{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt;\n",
    "\n",
    "# scientific\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 06:  Generative models and Logistic Regression\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  January 27, 2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "* Probabilistic Generative Models\n",
    "    - Gaussian Discriminant Analysis\n",
    "    - Naïve Bayes\n",
    "* Discriminant Functions\n",
    "    - Fisher’s linear discriminant\n",
    "    - Perceptron learning algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Generative Models\n",
    "\n",
    "## Learning the Classifier\n",
    "\n",
    "Our final goal is to learn the distribution $ P (C_k |  x)$ .\n",
    "\n",
    "(a) Discriminative Models: Directly model $ P (C_k |  x)$ and learn parameters from the training set.\n",
    "    \n",
    "(b) Generative Models: Learn class densities $ P (x |  C_k)$ and priors $ P(C_k) $. Use Bayes' theorem for $ P (C_k |  x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Bayes' Theorem reduces the classification problem $ P (C_k |  x)$ the distribution of the data.\n",
    "\n",
    "\n",
    "* Density estimation problems are easy to learn from labeled training data. \n",
    "    - $ P (C_k)$\n",
    "    - $ P (x | C_k)$\n",
    "\n",
    "\n",
    "* Maximum likelihood parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* For two classes, Bayes' theorem says:\n",
    "    $$ p(C_1|x) = \\frac {p(x|C_1) \\cdot p(C_1)} {p(x|C_1) \\cdot p(C_1) + p(x|C_2) \\cdot p(C_2)} $$\n",
    "    \n",
    "    \n",
    "* Use $ log $ odds:\n",
    "    $$ a = ln \\frac{p(C_1|x)}{p(C_2|x)} = ln \\frac{p(x|C_1) \\cdot p(C_1)}{p(x|C_2) \\cdot p(C_2)} $$\n",
    "    \n",
    "    \n",
    "* To define the posterior via $sigmoid$ :\n",
    "    $$ p(C_1|x) = \\frac {1} {1+ exp(-a)} = \\sigma(a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discriminative v. Generative: A comparison\n",
    "\n",
    "* The generative approach is typically model- based, and makes it possible to generate synthetic data from $ p(x|C_k) $.\n",
    "\n",
    "    – By comparing the synthetic data and real data, we get a sense of how good the generative model is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* The discriminative approach will typically have fewer parameters to estimate and have less assumptions about data distribution.\n",
    "\n",
    "    – Linear (e.g., logistic regression) versus quadratic (e.g. Gaussian discriminant analysis) in the dimension of the input.\n",
    "    \n",
    "    – Less generative assumptions about the data (however, constructing the features may need prior knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Discriminant Analysis\n",
    "\n",
    "* Prior Distribution \n",
    "    - $ p(C_k) $: assumed to be constant. (eg. Bernoulli)\n",
    "    \n",
    "    \n",
    "* Likelihood\n",
    "    - $ p(x| C_k) $: Gaussian distribution\n",
    "    \n",
    "    $ p (x| C_k) = \\frac{1}{(2\\pi) ^\\frac{D}{2} |\\Sigma|^ \\frac{1}{2}} exp \\left\\{- \\frac {1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) \\right\\} $\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Classification\n",
    "    - Use Bayes' Rule: For any class $i$,\n",
    "$$ p(C_i|x) = \\frac {p(x|C_i) \\cdot p(C_i)} {\\Sigma_k p(x|C_k) \\cdot p(C_k)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Code for Gaussian Discriminant analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Conditional Densities\n",
    "\n",
    "Suppose we model $ p(x |C_k) $ as Gaussians with the _same_ _covariance_ matrix:\n",
    "\n",
    "$ p (x| C_k) = \\frac{1}{(2\\pi) ^\\frac{D}{2} |\\Sigma|^ \\frac{1}{2}} exp \\left\\{- \\frac {1}{2} (x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) \\right\\} $\n",
    "\n",
    "This gives us $ p(C_1|x) = \\sigma(w^T x + w_0) $\n",
    "\n",
    "where $ w = \\Sigma^{-1}(\\mu_1 - \\mu_2) $ \n",
    "\n",
    "and $ w_0 = -\\frac{1}{2} \\mu_1^T \\Sigma ^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^T \\Sigma ^{-1} \\mu_2 + ln \\frac {p(C_1)}{p(C_2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P (x, C_1) \n",
    "&= P(x|C_1) \\cdot P(C_1) \\\\\n",
    "&= \\frac{1}{(2\\pi) ^\\frac{D}{2} |\\Sigma|^ \\frac{1}{2}} exp \\left\\{- \\frac {1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\right\\} \\cdot P(C_1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P (x, C_2) \n",
    "&= P(x|C_2) \\cdot P(C_2) \\\\\n",
    "&=  \\frac{1}{(2\\pi) ^\\frac{D}{2} |\\Sigma|^ \\frac{1}{2}} exp \\left\\{- \\frac {1}{2} (x - \\mu_2)^T \\Sigma^{-1} (x - \\mu_2) \\right\\} \\cdot P(C_2)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__\"Log Odds\"__\n",
    "$$\n",
    "\\begin{align}\n",
    "log \\frac {p(C_1|x)}{p(C_2|x)} \n",
    "&= log \\frac {p(C_1|x)}{1 - p(C_1|x)} \\\\\n",
    "&= log \\frac{exp \\left\\{- \\frac {1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\right\\}}{exp \\left\\{- \\frac {1}{2} (x - \\mu_2)^T \\Sigma^{-1} (x - \\mu_2) \\right\\}} + log \\frac{P(C_1)}{P(C_2)} \\\\\n",
    "&= \\left\\{- \\frac {1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\right\\} - \\left\\{- \\frac {1}{2} (x - \\mu_2)^T \\Sigma^{-1} (x - \\mu_2) \\right\\} +  log \\frac{P(C_1)}{P(C_2)} \\\\\n",
    "&= (\\mu_1 - \\mu_2)^T \\Sigma^{-1} x - \\frac{1}{2} \\mu_1\\Sigma^{-1}\\mu_1 + \\frac{1}{2} \\mu_2\\Sigma^{-1}\\mu_2 + log \\frac{P(C_1)}{P(C_2)} \\\\\n",
    "&= (\\Sigma^{-1}(\\mu_1 - \\mu_2))^T x + w_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $ w_0 = -\\frac{1}{2} \\mu_1^T \\Sigma ^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^T \\Sigma ^{-1} \\mu_2 + log \\frac {p(C_1)}{p(C_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class-Conditional Densities \n",
    "## (for shared covariances)\n",
    "\n",
    "* $ P(C_k|x) $ is a sigmoid function: $ \\sigma(a) = \\frac {1}{1+exp(-a)} $\n",
    "\n",
    "- with logg-odds (_logit function_):\n",
    "$ a = log(\\frac{\\sigma}{1-\\sigma} = (\\Sigma^{-1}(\\mu_1 - \\mu_2))^T x + w_0 $\n",
    "\n",
    "where $ w_0 = -\\frac{1}{2} \\mu_1^T \\Sigma ^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^T \\Sigma ^{-1} \\mu_2 + log \\frac {p(C_1)}{p(C_2)} $\n",
    "\n",
    "* Generalizes to normalized exponential or _softmax_: $ p_i = \\frac{exp(q_i)}{\\Sigma_j exp(q_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkhRTuIiIppHAXEUmh/wdoiPRuXpMHSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb082449210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sigmoid function\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Decision Boundaries\n",
    "\n",
    "* With the same covariance matrices, the boundary $ p(C_1|x) = p(C_2|x)$ is linear.\n",
    "    - Different priors $p(C_1), P(C_2)$ just shift it around.\n",
    "    \n",
    "TODO: More images here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning parameters via maximum likelihood\n",
    "\n",
    "* Given training data $\\{(x^{(1)}, t^{(1)}), .... , (x^{(N)}, t^{(N)})\\}$ and a generative model (_shared covariance_)\n",
    "\n",
    "$$ p(t) = \\phi^t (1-\\phi)^{(1-t)} $$\n",
    "\n",
    "$$ p(x|t=0) = \\frac{1}{\\sqrt{2\\pi} |\\Sigma|^\\frac{1}{2}} exp (- \\frac {1}{2} (x - \\mu_0)^T \\Sigma^{-1} (x - \\mu_0) ) $$\n",
    "\n",
    "$$ p(x|t=1) = \\frac{1}{\\sqrt{2\\pi} |\\Sigma|^\\frac{1}{2}} exp (- \\frac {1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) ) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__log likelihood__\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "log (L)  \n",
    "&= \\prod_{i=1}^{N} P(X^{(i)}, t^{(i)}) \\\\\n",
    "&= \\sum_{i=1}^{N} log P(X^{(i)}, t^{(i)})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning via maximum likelihood\n",
    "\n",
    "Maximum likelihood estimation solutions\n",
    "\n",
    "$$ \\phi = \\frac{1}{N} \\sum_{i=1}^{N} 1\\{t_i =1\\} $$\n",
    "\n",
    "$$ \\mu_0 = \\frac{\\sum_{i=1}^{N} 1\\{t_i = 0\\} x_i}{\\sum_{i=1}^{N} 1\\{t_i = 0\\}}  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ \\mu_1 = \\frac{\\sum_{i=1}^{N} 1\\{t_i = 1\\} x_i}{\\sum_{i=1}^{N} 1\\{t_i = 1\\}}  $$\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu_{t_i})(x_i - \\mu_{t_i})^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The case of Different Covariances\n",
    "\n",
    "* Decision boundaries can be quadratic.\n",
    "\n",
    "<img src=\"cov.png\" width=750px, align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GDA v. Logistic Regression\n",
    "\n",
    "* For an M-dimensional feature space, \n",
    "    - Logistic regression has to fit M parameters.\n",
    "    - GDA has to fit \n",
    "        * 2M parameters for means of $ p(x|C_1)$ and $ p(x|C_2)$\n",
    "        * M(M+1)/2 parameters for the shared covariance matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Logistic regression has less parameters and is more flexible about data distribution.\n",
    "\n",
    "\n",
    "* GDA has a stronger modeling assumption, and works well when the distribution follows the assumption."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
