{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\X}{\\mathcal{X}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\d}{\\mathrm{d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt;\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "from matplotlib.pylab import cm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# scientific\n",
    "import numpy as np;\n",
    "\n",
    "# ipython\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 12:  Information Theory and Exponential Families\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  March 7, 2016\n",
    "\n",
    "*Lecture Exposition Credit:*  Benjamin Bray & Saket Dewangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "- Information Theory\n",
    "    - Information, Entropy, Maximum Entropy Distributions\n",
    "    - Entropy and Encoding, Cross Entropy, Relative Entropy\n",
    "    - Mutual Information & Collocations\n",
    "    \n",
    "- Exponential Family\n",
    "    - Sufficient Statistic \n",
    "    - General Form of Exponential Family\n",
    "    - Likelihood and MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "- Required:\n",
    "    - **[PRML]**, §1.6: Information Theory\n",
    "    - **[PRML]**, §2.4: The Exponential Family   \n",
    "\n",
    "- Optional:    \n",
    "    - **[MLAPP]**, §2.8: Information Theory\n",
    "    - **[MLAPP]**, §9.2: Exponential Families   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other References\n",
    "\n",
    "- Information Theory:\n",
    "    - **[Shannon 1951]** Shannon, Claude E.. [*The Mathematical Theory of Communication*](http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf).  1951.\n",
    "    - **[Pierce 1980]** Pierce, John R..  [*An Introduction to Information Theory:  Symbols, Signals, and Noise*](http://www.amazon.com/An-Introduction-Information-Theory-Mathematics/dp/0486240614).  1980.\n",
    "    - **[Stone 2015]** Stone, James V..  [*Information Theory:  A Tutorial Introduction*](http://jim-stone.staff.shef.ac.uk/BookInfoTheory/InfoTheoryBookMain.html).  2015.\n",
    "\n",
    "- Exponential Families:\n",
    "    - **[MLAPP]** Murphy, Kevin. [*Machine Learning:  A Probabilistic Perspective*](https://mitpress.mit.edu/books/machine-learning-0).  2012.\n",
    "    - **[Hero 2008]** Hero, Alfred O..  [*Statistical Methods for Signal Processing*](http://web.eecs.umich.edu/~hero/Preprints/main_564_08_new.pdf).  2008.\n",
    "    - **[Blei 2011]** Blei, David. [*Notes on Exponential Families*](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/exponential-families.pdf).  2011.\n",
    "    - **[Wainwright & Jordan 2008]** Wainwright, Martin J. and Michael I. Jordan.  [*Graphical Models, Exponential Families, and Variational Inference*](https://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf).  2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> This lecture, we will not cover any classifier or regressor. Instead, some basics of information theory and exponential family will be introduced. These will provide some important background for **Probabilistic Graphical Models**, which is a big topic that we will cover for several following lectures. For information theory, some definitions like information, entropy, cross entropy, relative entropy, etc. are to be introduced. We could see how entropy is related to compression theory. As for applications, we will show how information theory can help us select features and find most frequent collocations in a novel. As for exponential family, we study it because it has some nice properties and will be frequently used in following lectures. Starting with definition of sufficient statistics, we will go through the general form, likelihood function and maximum likelihood estimator of exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information Theory\n",
    "\n",
    "> Uses material from **[MLAPP]** §2.8, **[Pierce 1980]**, **[Stone 2015]**, and **[Shannon 1951]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information Theory\n",
    "\n",
    "- Information theory is concerned with\n",
    "    - **Compression:**  Representing data in a compact fashion\n",
    "    - **Error Correction:**  Transmitting and storing data in a way that is robust to errors\n",
    "\n",
    "- In machine learning, information-theoretic quantities are useful for\n",
    "    - manipulating probability distributions\n",
    "    - interpreting statistical learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Information?\n",
    "\n",
    "- Can we measure the amount of **information** we gain from an observation?\n",
    "    - Information is measured in *bits* ( don't confuse with *binary digits*, $0110001\\dots$ )\n",
    "    - Intuitively, observing a fair coin flip should give 1 bit of information\n",
    "    - Observing two fair coins should give 2 bits, and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information:  Definition\n",
    "\n",
    "- The **information content** of an event $E$ with probability $p$ defined as\n",
    "    $$\n",
    "    I(E) = I(p) = - \\log_2 p = \\log_2 \\frac{1}{p} \\geq 0\n",
    "    $$\n",
    "\n",
    "    - Information theory is about *probabilities* and *distributions*\n",
    "    - The \"meaning\" of events doesn't matter.\n",
    "    - Using bases other than 2 yields different units (Hartleys, nats, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information Example:  Fair Coin—$P(\\text{Head})=0.5$\n",
    "\n",
    "- **One Coin:**  If we observe one head, then\n",
    "    $$\n",
    "    I(\\text{Head}) = - \\log_2 P(\\text{Head}) = 1 \\;\\mathrm{bit}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Two Coins:** If we observe two heads in a row, \n",
    "    $$\n",
    "    \\begin{align}\n",
    "    I(\\text{Head},\\text{Head})\n",
    "    &= -\\log_2 P(\\text{Head}, \\text{Head}) \\\\\n",
    "    &= -\\log_2 P(\\text{Head})P(\\text{Head}) \\\\\n",
    "    &= -\\log_2 P(\\text{Head}) - \\log_2 P(\\text{Head}) = 2 \\;\\mathrm{bits}\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Information Example:  Unfair Coin\n",
    "\n",
    "- Suppose the coin has two heads, so $P(\\text{Head})=1$.  Then,\n",
    "    $$\n",
    "    I(\\text{Head}) = - \\log_2 1 = 0\n",
    "    $$\n",
    "    - We will gain no information!\n",
    "- On the contrary, if we observe tail\n",
    "    $$\n",
    "    I(\\text{Tail}) = - \\log_2 0 = + \\infty\n",
    "    $$\n",
    "    - We will gain *infinite* information because we observe an impossible thing!\n",
    "\n",
    "- Information is a measure of how **surprised** we are by an outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy:  Definition\n",
    "\n",
    "- The **entropy** of a discrete random variable $X$ with distribution $p$ is\n",
    "    $$\n",
    "    H[X] = E[I(p(X))] = - \\sum_{x \\in X} p(x) \\log p(x)\n",
    "    $$    \n",
    "    - Entropy is the expected information received when we sample from $X$.\n",
    "    - Entropy measures how *surprised* we are on average\n",
    "    - When $X$ is continuous random variable, summation is replaced with integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy:  Coin Flip\n",
    "\n",
    "- If $X$ is binary, entropy is\n",
    "    $$\n",
    "    H[X] = -p \\log p + (1-p) \\log (1-p)\n",
    "    $$\n",
    "    \n",
    "<center>\n",
    "<div class=\"image\"   style=\"width:551px\">\n",
    "    <img src=\"images/Entropy_Plot.png\">\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "- Entropy is highest when $X$ is close to uniform.\n",
    "    - Large entropy $\\iff$ high uncertainty, more information from each new observation\n",
    "    - Small entropy $\\iff$ more knowledge about possible outcomes\n",
    "\n",
    "- The farther from uniform $X$ is, the smaller the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Entropy Principle\n",
    "\n",
    "- Suppose we sample data from an unknown distribution $p$, and\n",
    "    - we collect statistics (mean, variance, etc.) from the data\n",
    "    - we want an *objective* or unbiased estimate of $p$\n",
    "    The **Maximum Entropy Principle** states that:\n",
    "\n",
    "> We should choose $p$ to have maximum entropy $H[p]$ among all distributions satisfying our constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some examples of maximum entropy distributions:\n",
    "\n",
    "<table>\n",
    "<thead><th>Constraints</th><th>Maximum Entropy Distribution</th></thead>\n",
    "<tbody>\n",
    "    <tr><td>Min $a$, Max $b$</td><td>Uniform $U[a,b]$</td></tr>\n",
    "    <tr><td>Mean $\\mu$, Support $(0,+\\infty)$</td><td>Exponential $Exp(\\mu)$</td></tr>\n",
    "    <tr><td>Mean $\\mu$, Variance $\\sigma^2$</td><td>Gaussian $\\mathcal{N}(\\mu, \\sigma^2)$</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "- Later, **Exponential Family Distributions** will generalize this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy and Encoding: Communication Channel\n",
    "\n",
    "- Now let's see how entropy is related to encoding theory\n",
    "- **Communication channel** can be characterized as:\n",
    "    - **[Source]** generates messages.\n",
    "    - **[Encoder]** converts the message to a **signal** for transmission.\n",
    "    - **[Channel]** is the path along which signals are transmitted, possibly under the influence of **noise**.\n",
    "    - **[Decoder[** attempts to reconstruct the original message from the transmitted signal.\n",
    "    - **[Destination]** is the intended recipient.\n",
    "<center>\n",
    "<div class=\"image\"   style=\"width:700px\">\n",
    "    <img src=\"images/communication.jpg\">\n",
    "</div>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy and Encoding: Encoding\n",
    "\n",
    "- Suppose we draw messages from a distribution $p$.\n",
    "    - Certain messages may be more likely than others.\n",
    "    - For example, the letter **e** is most frequent in English\n",
    "\n",
    "- An **efficient** encoding minimizes the average code length,\n",
    "    - assign *short* codewords to common messages\n",
    "    - and *longer* codewords to rare messages\n",
    "    \n",
    "- Example: **Morse Code**\n",
    "<center>\n",
    "<div class=\"image\"   style=\"width:450px\">\n",
    "    <img src=\"images/morse-code.jpg\">\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy and Encoding: Source Coding Theorem\n",
    "\n",
    "- Claude Shannon proved that for discrete noiseless channels:\n",
    "\n",
    "> It is impossible to encode messages drawn from a distribution $p$ with fewer than $H[p]$ bits, on average.\n",
    "\n",
    "- Here, *bits* refers to *binary digits*, i.e. encoding messages in binary.\n",
    "\n",
    "> $H[p]$ measures the optimal code length, in bits, for messages drawn from $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Entropy & Relative Entropy\n",
    "\n",
    "- Consider different distributions $p$ and $q$\n",
    "    - What if we use a code optimal for $q$ to encode messages from $p$?\n",
    "\n",
    "- For example, suppose our encoding scheme is optimal for German text.\n",
    "    - What if we send English messages instead?\n",
    "    - Certainly, there will be some waste due to different letter frequencies, umlauts, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross Entropy & Relative Entropy\n",
    "\n",
    "- **Cross entropy** measures the average number of bits needed to encode messages drawn from $p$ when we use a code optimal for $q$:\n",
    "    $$\n",
    "    H(p,q) = -\\sum_{x \\in \\X} p(x) \\log q(x)\n",
    "    = E_p[\\log q(x)]\n",
    "    $$\n",
    "\n",
    "- Intuitively, $H(p,q) \\geq H(p)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Relative entropy** is the difference $H(p,q) - H(p)$.\n",
    "\n",
    "- Relative entropy, aka **Kullback-Leibler divergence**, of $q$ from $p$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    D_{KL}(p \\| q)\n",
    "    &= H(p,q) - H(p) \\\\\n",
    "    &= \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\\\\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "> Measures the number of *extra* bits needed to encode messages from $p$ if we use a code optimal for $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutual Information:  Definition\n",
    "\n",
    "- **Mutual information** between discrete variables $X$ and $Y$ is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    I(X; Y)\n",
    "    &= \\sum_{y\\in Y} \\sum_{x \\in X} p(x,y) \\log\\frac{p(x,y)}{p(x)p(y)} \\\\\n",
    "    &= D_{KL}( p(x,y) \\| p(x)p(y) )\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "    - If $X$ and $Y$ are independent, $p(x,y)=p(x)p(y)$ and $I(X; Y)=0$\n",
    "    - So, $I(X;Y)$ measures how *dependent* $X$ and $Y$ are!\n",
    "    - Related to correlation $\\rho(X,Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mutual Information: Example of Feature Selection\n",
    "\n",
    "- Mutual information can also be used for **feature selection**.\n",
    "    - In classification, features that *depend* most on the class label $C$ are useful\n",
    "    - So, choose features $X_k$ such that $I(X_k ; C)$ is large\n",
    "    - This helps to avoid *overfitting* by ignoring irrelevant features!\n",
    "\n",
    "> See **[MLAPP]** §3.5.4 for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointwise Mutual Information\n",
    "\n",
    "- A **collocation** is a sequence of words that co-occur more often than expected by chance.\n",
    "    - fixed expression familiar to native speakers (hard to translate)\n",
    "    - meaning of the whole is more than the sum of its parts\n",
    "    - See [these slides](https://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/philip-pmi.pdf) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Substituting a synonym sounds unnatural:\n",
    "    - \"fast food\" vs. \"quick food\"\n",
    "    - \"Great Britain\" vs. \"Good Britain\"\n",
    "    - \"warm greetings\" vs \"hot greetings\"\n",
    "\n",
    "- How can we find collocations in a corpus of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointwise Mutual Information\n",
    "\n",
    "- The **pointwise mutual information (PMI)** between words $x$ and $y$ is\n",
    "    $$\n",
    "    \\mathrm{pmi}(x;y) = \\log \\frac{p(x,y)}{p(x)p(y)}\n",
    "    $$\n",
    "\n",
    "    - $p(x)p(y)$ is how frequently we **expect** $x$ and $y$ to co-occur, if $x$ and $y$ are independent.\n",
    "    - $p(x,y)$ measures how frequently $x$ and $y$ **actually** occur together\n",
    "    \n",
    "- **Idea:**  Rank word pairs by $\\mathrm{pmi}(x,y)$ to find collocations!\n",
    "    - $\\mathrm{pmi}(x,y)$ is large if $x$ and $y$ co-occur more frequently together than expected\n",
    "\n",
    "- **Example:** Let's try it on the novel *Crime and Punishment*!\n",
    "    - Pre-computed unigram and bigram counts are found in the `collocations/data` folder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Here we read in the precomputed data.\n",
    "\n",
    "import csv, math;\n",
    "\n",
    "# file paths\n",
    "unigram_path = \"collocations/data/crime-and-punishment.txt.unigrams\";\n",
    "bigram_path = \"collocations/data/crime-and-punishment.txt.bigrams\";\n",
    "\n",
    "# read unigrams into dict\n",
    "with open(unigram_path) as f:\n",
    "    reader = csv.reader(f);\n",
    "    unigrams = { row[0] : int(row[1]) for row in csv.reader(f)};\n",
    "    \n",
    "# read bigrams into dict\n",
    "with open(bigram_path) as f:\n",
    "    reader = csv.reader(f);\n",
    "    bigrams = { (row[0],row[1]) : int(row[2]) for row in csv.reader(f)};\n",
    "     \n",
    "# pretty print table\n",
    "class PrettyTable(object):\n",
    "        def __init__(self, data, head1, head2, floats=False):\n",
    "            table = \"<table>\"      \n",
    "            \n",
    "            table += \"<tr><th>%s</th>\" % head1;\n",
    "            for bigram, count in data:\n",
    "                table +=\"<td>%s %s</td>\" %bigram\n",
    "            table += \"</tr>\"\n",
    "            \n",
    "            table += \"<tr><th>%s</th>\" % head2;\n",
    "            for bigram, count in data:\n",
    "                if floats: count = \"%0.2f\" % count;\n",
    "                else: count = \"%d\" % count;                \n",
    "                table +=\"<td>%s</td>\" % count\n",
    "            table += \"</tr>\"\n",
    "            table += \"</table>\"\n",
    "            self.table = table;            \n",
    "        \n",
    "        def _repr_html_(self):\n",
    "            return self.table;        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The following code sorts bigrams by pointwise mutual information:\n",
    "\n",
    "# compute pmi\n",
    "pmi_bigrams = [];\n",
    "\n",
    "for w1,w2 in bigrams:\n",
    "    # compute pmi\n",
    "    actual = bigrams[(w1,w2)];\n",
    "    expected = unigrams[w1] * unigrams[w2];\n",
    "    pmi = math.log( actual / expected );\n",
    "    # filter out infrequent bigrams\n",
    "    if actual < 15: continue;\n",
    "    pmi_bigrams.append( ((w1, w2), pmi) );\n",
    "\n",
    "# sort pmi\n",
    "pmi_sorted = sorted(pmi_bigrams, key=lambda x: x[1], reverse=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointwise Mutual Information: Example\n",
    "\n",
    "- Here are the most frequent bigrams--these aren't collocations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Bigram</th><td>in the</td><td>of the</td><td>he was</td><td>he had</td><td>to the</td><td>on the</td><td>i am</td><td>at the</td><td>it was</td><td>that he</td></tr><tr><th>Count</th><td>778</td><td>598</td><td>505</td><td>498</td><td>488</td><td>479</td><td>460</td><td>459</td><td>413</td><td>335</td></tr></table>"
      ],
      "text/plain": [
       "<__main__.PrettyTable_ at 0x996c400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_sorted = sorted(bigrams.items(), key=lambda x: x[1], reverse=True);\n",
    "PrettyTable(bigrams_sorted[:10], \"Bigram\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sorting bigrams by PMI, we first get names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Collocation</th><td>andrey semyonovitch</td><td>nikodim fomitch</td><td>hay market</td><td>dmitri prokofitch</td><td>honoured sir</td><td>sofya semyonovna</td><td>marfa petrovna</td><td>police station</td><td>rodion romanovitch</td></tr><tr><th>PMI</th><td>-3.18</td><td>-3.18</td><td>-3.48</td><td>-3.87</td><td>-4.27</td><td>-4.33</td><td>-4.37</td><td>-4.48</td><td>-4.57</td></tr></table>"
      ],
      "text/plain": [
       "<__main__.PrettyTable_ at 0x996ce48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrettyTable(pmi_sorted[1:10], \"Collocation\", \"PMI\", floats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ...then more interesting collocations!  This is much more useful than sorting by frequency alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Collocation</th><td>thank god</td><td>police office</td><td>great deal</td><td>ten minutes</td><td>good heavens</td><td>thousand roubles</td><td>katerina ivanovnas</td><td>old womans</td></tr><tr><th>PMI</th><td>-5.20</td><td>-5.23</td><td>-5.28</td><td>-5.40</td><td>-5.51</td><td>-5.54</td><td>-5.57</td><td>-5.57</td></tr></table>"
      ],
      "text/plain": [
       "<__main__.PrettyTable_ at 0x996cba8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrettyTable(pmi_sorted[12:20], \"Collocation\", \"PMI\", floats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exponential Families\n",
    "\n",
    "> Uses material from **[MLAPP]** §9.2 and **[Hero 2008]** §3.5, §4.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family: Introduction\n",
    "\n",
    "- We have seen many distributions.\n",
    "    - Bernoulli\n",
    "    - Gaussian\n",
    "    - Exponential\n",
    "    - Gamma \n",
    "    \n",
    "- Many of these belong to a more general class called the **exponential family**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why do we care?\n",
    "    - only family of distributions with finite-dimensional **sufficient statistics**\n",
    "    - only family of distributions for which **conjugate priors** exist\n",
    "    - makes the least set of assumptions subject to some user-chosen constraints (**Maximum Entropy**)\n",
    "    - core of generalized linear models and **variational inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics:  Definition\n",
    "\n",
    "- **Recall:** A **statistic** $T(\\D)$ is a function of the observed data $\\D$.\n",
    "    - Mean, $T(x_1, \\dots, x_n) = \\frac{1}{n}\\sum_{k=1}^n x_k$\n",
    "    - Variance, maximum, mode, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we have some distribution with parameters $\\theta$.  Then,\n",
    "\n",
    "> A statistic $T(\\D)$ is **sufficient** for $\\theta$ if no other statistic calculated from the same sample provides any additional information about $\\theta$.\n",
    "\n",
    "- Mathematically,\n",
    "    $$\n",
    "    P(\\theta\\, | \\, \\D, T(\\D)) = P(\\theta\\, | \\,T(\\D))\n",
    "    $$\n",
    "    Given statistic $T(\\D)$, $\\theta$ is independent of data $\\D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics:  Example\n",
    "\n",
    "- Suppose $X \\sim \\mathrm{Bernoulli}(\\theta)$, i.e. $P(X=1)=\\theta, P(X=0)=1-\\theta$ and we observe $\\mathcal{D} = \\{x_1, \\dots, x_N\\} \\in \\{0,1\\}^N$ \n",
    "- Then statistic $T(\\D) = \\frac{1}{N} \\sum_{n=1}^N x_n$, i.e. number of occurrence, is *sufficient* for $\\theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Proof for sufficiency**\n",
    "    - Let $\\tau = T(\\D)$, we have\n",
    "        $$\n",
    "        \\begin{split}\n",
    "        P(\\D \\, | \\, \\theta) \n",
    "        &= P(\\D, \\tau\\, | \\, \\theta) \\\\\n",
    "        &= \\theta^\\tau (1-\\theta)^{N-\\tau} \n",
    "        \\end{split}\n",
    "        \\qquad\n",
    "        P(\\tau\\, | \\, \\theta ) = \\binom{N}{\\tau} \\theta^\\tau (1-\\theta)^{N-\\tau} \\qquad\n",
    "        P(\\D \\, | \\, \\tau) = 1 \\Big/ \\binom{N}{\\tau}\n",
    "        $$\n",
    "        Therefore,\n",
    "        $$\n",
    "        P(D, \\tau \\, | \\, \\theta) = P(\\tau\\, | \\, \\theta )P(D \\, | \\, \\tau)\n",
    "        $$\n",
    "    \n",
    "    - For $P(\\theta \\, | \\, \\D, \\tau)$, we have\n",
    "        $$\n",
    "        \\begin{split}\n",
    "        P(\\theta \\, | \\, \\D, \\tau) \n",
    "        &= \\frac{P(\\D, \\tau \\, | \\, \\theta)P(\\theta)}{P(\\D, \\tau)} = \\frac{P(\\tau\\, | \\, \\theta )P(D \\, | \\, \\tau)P(\\theta)}{P(\\D, \\tau)} \\\\\n",
    "        &= \\frac{P(\\tau\\, | \\, \\theta )P(D \\, | \\, \\tau)P(\\theta)}{P(\\D\\, | \\, \\tau) P(\\tau)} = \\frac{P(\\tau\\, | \\, \\theta )P(\\theta)}{P(\\tau)} \\\\ \n",
    "        &= P(\\theta \\, | \\, \\tau) \\qquad \\mathbf{Q.E.D.}\n",
    "        \\end{split} \\\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Definition\n",
    "\n",
    "- $p(x \\,|\\, \\theta)$ has **exponential family form** if:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    p(x \\,|\\, \\theta)\n",
    "    &= \\frac{1}{Z(\\theta)} h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) \\right] \\\\\n",
    "    &= h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) - A(\\theta) \\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    of which $p(x \\,|\\, \\theta)$ means *distribution of $x$ parameterized by $\\theta$*\n",
    "\n",
    "    - $Z(\\theta) = \\int h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) \\right] \\d x$ is the **partition function** for normalization\n",
    "    - $A(\\theta) = \\log Z(\\theta)$ is the **log partition function**\n",
    "    - $\\phi(x) \\in \\R^d$ is a vector of **sufficient statistics**\n",
    "    - $\\eta(\\theta)$ maps $\\theta$ to a set of **natural parameters**\n",
    "    - $h(x)$ is a scaling constant, usually $h(x)=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Example—Bernoulli\n",
    "\n",
    "- The Bernoulli distribution can be written as\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathrm{Bern}(x \\,|\\, \\theta)\n",
    "    &= \\theta^x (1-\\theta)^{1-x} \\\\\n",
    "    &= \\exp\\left[ x \\log \\theta + (1-x) \\log (1-\\theta) \\right] \\\\\n",
    "    &= \\exp\\left[ \\eta(\\theta)^T \\phi(x) \\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\eta(\\theta) = (\\log\\theta, \\log(1-\\theta))$ and $\\phi(x) = (x, 1-x)$\n",
    "    - There is a linear dependence between features $\\phi(x)$\n",
    "    - This representation is **overcomplete**\n",
    "    - $\\eta$ is not uniquely determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- Instead, we can find a **minimal** parameterization:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathrm{Ber}(x \\,|\\, \\theta) \n",
    "    &= (1-\\theta) \\exp\\left[ x \\log\\frac{\\theta}{1-\\theta} \\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "\n",
    "- This gives **natural parameters** $\\eta(\\theta) = \\log \\frac{\\theta}{1-\\theta}$.\n",
    "    - Now, $\\eta$ is unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Example—Gaussian\n",
    "- The Gaussian distribution can be written as\n",
    "    $$\n",
    "    \\begin{split}\n",
    "    \\mathcal{N}(x \\,|\\, \\mu, \\sigma^2) \n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left\\{ - \\frac{(x-\\mu)^2}{2 \\sigma^2} \\right\\} \\\\\n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left\\{ -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} + \\frac{x\\mu}{2\\sigma^2}\\right\\} \\\\\n",
    "    &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ - \\frac{\\mu^2}{2\\sigma^2} \\right\\} \\exp \\left\\{ \\begin{bmatrix} -\\frac{1}{2\\sigma^2} & \\frac{\\mu}{\\sigma^2} \\end{bmatrix} \\begin{bmatrix} x^2 \\\\ x \\end{bmatrix} \\right\\}\n",
    "    \\end{split}\n",
    "    $$\n",
    "    of which\n",
    "    $$\n",
    "    \\frac{1}{Z(\\theta)} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ - \\frac{\\mu^2}{2\\sigma^2} \\right\\} \\qquad \n",
    "    \\eta(\\theta) = \\begin{bmatrix} -\\frac{1}{2\\sigma^2} \\\\ \\frac{\\mu}{\\sigma^2} \\end{bmatrix} \\qquad\n",
    "    \\phi(x) = \\begin{bmatrix} x^2 \\\\ x \\end{bmatrix}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Example—Others\n",
    "\n",
    "-   Exponential Family Distributions:\n",
    "    - Multivariate normal\n",
    "    - Exponential\n",
    "    - Dirichlet\n",
    "\n",
    "-   Non-examples:\n",
    "    - Student t-distribution can't be written in exponential form\n",
    "    - Uniform distribution support depends on the parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family: Notation Change\n",
    "\n",
    "- Recall our exponential family has the form\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    p(x \\,|\\, \\theta)\n",
    "    &= \\frac{1}{Z(\\theta)} h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) \\right] = h(x) \\exp\\left[ \\eta(\\theta)^T \\phi(x) - A(\\theta) \\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    of which natural parameter is $\\eta(\\theta)$.\n",
    "    \n",
    "- Now we change the notation a little bit\n",
    "    - let $\\theta$ denote **natural parameter**, i.e. replace $\\eta(\\theta)$ with $\\theta$, so that we could manipulate natural parameter directly. So we have a new form of exponential family\n",
    "        $$\n",
    "        p(x \\,|\\, \\theta) = \\frac{1}{Z(\\theta)} h(x) \\exp\\left[ \\theta^T \\phi(x) \\right] = h(x) \\exp\\left[ \\theta^T \\phi(x) - A(\\theta) \\right]\n",
    "        $$\n",
    "    - Note that this new function $Z(\\theta)$ and $A(\\theta)$ is different from old $Z(\\theta)$ and $A(\\theta)$ because we have changed the notation of $\\theta$\n",
    "        \n",
    "\n",
    "- After this notation change, we have log-partition function:\n",
    "    $$\n",
    "    A(\\theta) = \\log Z(\\theta) = \\log \\int  h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family: Log-partition Function\n",
    "\n",
    "- Recall our log-partition function is \n",
    "    $$\n",
    "    A(\\theta) = \\log \\int  h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x\n",
    "    $$\n",
    "\n",
    "- Derivatives of **log-partition function** $A(\\theta)$ yield **cumulants** of  sufficient statistics (Proof in the note)\n",
    "    - $\\nabla_\\theta A(\\theta) = E\\left[\\phi(x)\\right]$\n",
    "    - $\\nabla^2_\\theta A(\\theta) = Cov[ \\phi(x) ]$\n",
    "    \n",
    "- Since covariance $Cov[ \\phi(x) ]$ is positive definite,i.e. $Cov[ \\phi(x) ] \\succ 0$, we have\n",
    "    - $\\nabla^2_\\theta A(\\theta)$ is positive definite\n",
    "    - and $A(\\theta)$ is *strictly convex*!\n",
    "\n",
    "- Later, we will see this could guarantee a unique global maximum of the likelihood $P(\\D\\,|\\, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> **Remark**\n",
    "> - Proof of Convexity: **First Derivative**\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\d A(\\theta)}{\\d \\theta}\n",
    "&= \\frac{\\d}{\\d\\theta} \\left[ \\log \\int  h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x \\right] \\\\\n",
    "&= \\frac{\\frac{\\d}{\\d\\theta} \\int  h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x}{\\int  h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x} \\\\\n",
    "&= \\frac{\\int  \\phi(x) h(x) \\exp\\left[\\theta^T \\phi(x) \\right] \\d x}{\\exp\\left[ A(\\theta) \\right]} \\\\\n",
    "&= \\int \\phi(x) \\underbrace{ h(x) \\exp \\left[ \\theta^T \\phi(x)-A(\\theta) \\right] }_{p(x)} \\d x \\\\\n",
    "&= \\int \\phi(x) p(x) dx = E[\\phi(x)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> - Proof of Convexity: **Second Derivative**\n",
    "\n",
    "> - Recall we just derived\n",
    "    $$\n",
    "    \\frac{\\d A(\\theta)}{\\d \\theta} = \\int \\phi(x) h(x) \\exp \\left[ \\theta^T \\phi(x)-A(\\theta) \\right] \\d x\n",
    "    $$\n",
    "    So the second derivative is\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\frac{\\d^2A}{\\d\\theta^2}\n",
    "    & = \\int \\phi(x) h(x) \\exp \\left[ \\theta^T \\phi(x)-A(\\theta) \\right]\\left[ \\phi(x) - \\frac{\\d A(\\theta)}{\\d \\theta} \\right] \\d x \\\\\n",
    "    & = \\int \\phi(x) p(x) \\left[ \\phi(x) - \\frac{\\d A(\\theta)}{\\d \\theta} \\right] \\d x \\\\\n",
    "    & = \\int \\phi^2(x) p(x) \\d x - \\frac{\\d A(\\theta)}{\\d \\theta} \\int \\phi(x)p(x) \\d x \\\\\n",
    "    & = E[\\phi^2(x)] - E[\\phi(x)]^2  \\hspace{2em}   (\\because \\d A(\\theta)/\\d \\theta = E[\\phi(x)])  \\\\ \n",
    "    & = Var[\\phi(x)]\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "> - For multi-variate case, we have \n",
    "    $$\n",
    "    \\frac{\\partial^2A}{\\partial\\theta_i \\partial\\theta_j} = E[\\phi_i(x)\\phi_j(x)] - E[\\phi_i(x)] E[\\phi_j(x)]\n",
    "    $$\n",
    "    and hence,\n",
    "    $$ \n",
    "    \\nabla^2A(\\theta) = Cov[\\phi(x)] \n",
    "    $$\n",
    "    Since covariance is positive definite, we have $A(\\theta)$ strictly convex as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  Likelihood\n",
    "\n",
    "- For single data $x_n$, its likelihood is\n",
    "    $$  \n",
    "    p(x_n \\,|\\, \\theta)= h(x_n) \\exp\\left[ \\theta^T \\phi(x_n) - A(\\theta)\\right]\n",
    "    $$\n",
    "  \n",
    "\n",
    "- For data $\\D = \\{ x_1, \\dots, x_N \\}$, the likelihood is\n",
    "    $$\n",
    "    p(\\D \\,|\\, \\theta)\n",
    "    = \\left[ \\prod \\nolimits_{n=1}^N h(x_n) \\right] \\exp\\left[ \\theta^T \\sum \\nolimits_{n=1}^N \\phi(x_n) - NA(\\theta)\\right]\n",
    "    $$\n",
    "\n",
    "- The sufficient statistics are now $\\phi(\\D) = \\sum_{n=1}^N \\phi(x_n)$.\n",
    "    - **Bernoulli:** $\\phi(\\D) = \\sum_{n=1}^N x_n$\n",
    "    - **Normal:** $\\phi(\\D) = [ \\sum_n x_n, \\sum_n x_n^2 ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The log-likelihood is (we have omitted terms independent of $\\theta$ )\n",
    "    $$\n",
    "    \\log p(\\D\\,|\\,\\theta) = \\theta^T \\phi(\\D) - N A(\\theta)\n",
    "    $$\n",
    "    \n",
    "- Since $-A(\\theta)$ is *strictly concave* and $\\theta^T\\phi(\\D)$ *linear* w.r.t $\\theta$,\n",
    "    - the log-likelihood is *strictly concave*\n",
    "    - there is a *unique* global maximum for likelihood!\n",
    "    - we have *unique* **maximum likelihood estimate (MLE)** for $\\theta$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  MLE\n",
    "\n",
    "- At the MLE $\\hat\\theta_{MLE}$, we have \n",
    "    $$\n",
    "    \\nabla_\\theta \\log p(\\D \\,|\\, \\theta)=0\n",
    "    $$\n",
    "    \n",
    "- For the derivative of log-likelihood, we have\n",
    "    $$\n",
    "    \\nabla_\\theta \\log p(\\D \\,|\\, \\theta) = \\nabla_\\theta \\left[ \\theta^T \\phi(\\D) - N A(\\theta) \\right] \\overset{\\nabla_\\theta A(\\theta) = E[\\phi(x)]}{=} \\phi(\\D) - N E[\\phi(X)]\\\\\n",
    "    $$\n",
    "- In conclusion, at the MLE $\\hat\\theta_{MLE}$ we have\n",
    "    $$\n",
    "    E[\\phi(x)] = \\frac{\\phi(\\D)}{N} = \\frac{1}{N} \\sum \\nolimits_{n=1}^N \\phi(x_n)\n",
    "    $$\n",
    "    - Expected value (parameterized by $\\theta$) of sufficient statistics equals empirical average of them when $\\theta = \\hat\\theta_{MLE}$\n",
    "    - This is called **moment matching**\n",
    "    - We could obtain MLE in this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exponential Family:  MLE—Bernoulli\n",
    "\n",
    "- Recall we just showed for MLE $\\hat\\theta_{MLE}$, we have\n",
    "    $$\n",
    "    E[\\phi(X)] = \\frac{1}{N} \\sum \\nolimits_{n=1}^N \\phi(x_n)\n",
    "    $$\n",
    "    \n",
    "- For $\\mathrm{Bernoulli}(\\theta)$, we know\n",
    "    $$\n",
    "    E[\\phi(X)] = E[x] = \\theta\n",
    "    $$\n",
    "    and we have showed\n",
    "    $$\n",
    "    \\phi(x) = x\n",
    "    $$\n",
    "\n",
    "- So the MLE $\\hat\\theta_{MLE}$ can be obtained by\n",
    "    $$\n",
    "    \\hat\\theta_{MLE} = \\frac{1}{N} \\sum \\nolimits_{n=1}^N x_n\n",
    "    $$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
